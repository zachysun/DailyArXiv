---
title: Latest 20 Papers - November 26, 2025
labels: documentation
---
## LLM
| **Title** | **Date** | **Cool Paper** | **Comment** |
| --- | --- | --- | --- |
| **[Cost-Aware Contrastive Routing for LLMs](https://arxiv.org/abs/2508.12491v3)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2508.12491v3) |  |
| **[Cognitive Foundations for Reasoning and Their Manifestation in LLMs](https://arxiv.org/abs/2511.16660v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.16660v2) | <details><summary>40 pa...</summary><p>40 pages, 4 tables, 6 figures</p></details> |
| **[Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design](https://arxiv.org/abs/2511.19423v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19423v1) | 10 pages, 4 figures |
| **[Information Extraction From Fiscal Documents Using LLMs](https://arxiv.org/abs/2511.10659v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.10659v2) | <details><summary>6 pag...</summary><p>6 pages. Presented at the AI for Financial Inclusion, Risk Modeling and Resilience in Emerging Markets workshop at ACM ICAIF 2025 Singapore</p></details> |
| **[Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification](https://arxiv.org/abs/2510.03469v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2510.03469v2) | <details><summary>Accep...</summary><p>Accepted to AgenticSE Workshop at ASE 2025</p></details> |
| **[ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework](https://arxiv.org/abs/2510.03463v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2510.03463v2) | <details><summary>Accep...</summary><p>Accepted to MAS-GAIN Workshop at ASE 2025</p></details> |
| **[LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems](https://arxiv.org/abs/2511.19368v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19368v1) | 15 pages, 9 figures |
| **[LLM Agents for Automated Dependency Upgrades](https://arxiv.org/abs/2510.03480v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2510.03480v2) | <details><summary>Accep...</summary><p>Accepted to AISM Workshop at ASE 2005</p></details> |
| **[Leveraging LLMs for reward function design in reinforcement learning control tasks](https://arxiv.org/abs/2511.19355v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19355v1) |  |
| **[Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces](https://arxiv.org/abs/2511.19333v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19333v1) |  |
| **[Enhancing Domain-Specific Encoder Models with LLM-Generated Data: How to Leverage Ontologies, and How to Do Without Them](https://arxiv.org/abs/2503.22006v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2503.22006v2) | <details><summary>Publi...</summary><p>Published in the Findings of the Association for Computational Linguistics: EMNLP 2025</p></details> |
| **[How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective](https://arxiv.org/abs/2505.21505v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2505.21505v2) | AAAI 2026 (Oral) |
| **[WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making](https://arxiv.org/abs/2506.06725v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2506.06725v2) |  |
| **[Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization](https://arxiv.org/abs/2511.19218v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19218v1) |  |
| **[LLM-Based Agentic Negotiation for 6G: Addressing Uncertainty Neglect and Tail-Event Risk](https://arxiv.org/abs/2511.19175v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19175v1) | <details><summary>Link ...</summary><p>Link to open-source non-commercial code available</p></details> |
| **[Can LLMs Threaten Human Survival? Benchmarking Potential Existential Threats from LLMs via Prefix Completion](https://arxiv.org/abs/2511.19171v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19171v1) |  |
| **[AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking](https://arxiv.org/abs/2506.07751v3)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2506.07751v3) | Under review |
| **[URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training](https://arxiv.org/abs/2505.16570v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2505.16570v2) | <details><summary>NeurI...</summary><p>NeurIPS 2025, Camera Ready</p></details> |
| **[Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?](https://arxiv.org/abs/2505.07078v4)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2505.07078v4) | <details><summary>Accep...</summary><p>Accepted to KDD 2026, Datasets & Benchmarks Track</p></details> |
| **[LLMs-Powered Real-Time Fault Injection: An Approach Toward Intelligent Fault Test Cases Generation](https://arxiv.org/abs/2511.19132v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19132v1) |  |

## Multimodal
| **Title** | **Date** | **Cool Paper** | **Comment** |
| --- | --- | --- | --- |
| **[UniGame: Turning a Unified Multimodal Model Into Its Own Adversary](https://arxiv.org/abs/2511.19413v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19413v1) |  |
| **[ReMatch: Boosting Representation through Matching for Multimodal Retrieval](https://arxiv.org/abs/2511.19278v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19278v1) |  |
| **[A Survey of Generative Categories and Techniques in Multimodal Generative Models](https://arxiv.org/abs/2506.10016v3)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2506.10016v3) |  |
| **[Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation](https://arxiv.org/abs/2511.19257v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19257v1) | <details><summary>Accep...</summary><p>Accepted at KDD 2026 First Cycle (full version). Authors marked with * contributed equally. Yi Liu is the lead author</p></details> |
| **[From Raw Features to Effective Embeddings: A Three-Stage Approach for Multimodal Recipe Recommendation](https://arxiv.org/abs/2511.19176v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19176v1) |  |
| **[OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs](https://arxiv.org/abs/2511.19023v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19023v1) |  |
| **[UMCL: Unimodal-generated Multimodal Contrastive Learning for Cross-compression-rate Deepfake Detection](https://arxiv.org/abs/2511.18983v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18983v1) | <details><summary>24-pa...</summary><p>24-page manuscript accepted to IJCV</p></details> |
| **[M2R2: MultiModal Robotic Representation for Temporal Action Segmentation](https://arxiv.org/abs/2504.18662v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2504.18662v2) | <details><summary>8 pag...</summary><p>8 pages, 6 figures, 2 tables</p></details> |
| **[VADE: Variance-Aware Dynamic Sampling via Online Sample-Level Difficulty Estimation for Multimodal RL](https://arxiv.org/abs/2511.18902v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18902v1) |  |
| **[Parallel Vision Token Scheduling for Fast and Accurate Multimodal LMMs Inference](https://arxiv.org/abs/2511.18875v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18875v1) |  |
| **[GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction](https://arxiv.org/abs/2511.18874v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18874v1) |  |
| **[UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model](https://arxiv.org/abs/2511.18845v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18845v1) |  |
| **[Assessing the alignment between infants' visual and linguistic experience using multimodal language models](https://arxiv.org/abs/2511.18824v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18824v1) |  |
| **[VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models](https://arxiv.org/abs/2511.18823v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18823v1) |  |
| **[ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection](https://arxiv.org/abs/2511.18780v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18780v1) |  |
| **[Robust Multimodal Sentiment Analysis with Distribution-Based Feature Recovery and Fusion](https://arxiv.org/abs/2511.18751v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18751v1) | <details><summary>Accep...</summary><p>Accepted by ACM MM 2024</p></details> |
| **[Multimodal Large Language Models with Adaptive Preference Optimization for Sequential Recommendation](https://arxiv.org/abs/2511.18740v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18740v1) | 11 pages,6 figures |
| **[SoK: The Security-Safety Continuum of Multimodal Foundation Models through Information Flow and Global Game-Theoretic Analysis of Asymmetric Threats](https://arxiv.org/abs/2411.11195v5)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2411.11195v5) |  |
| **[AIRHILT: A Human-in-the-Loop Testbed for Multimodal Conflict Detection in Aviation](https://arxiv.org/abs/2511.18718v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18718v1) | <details><summary>9 pag...</summary><p>9 pages, 4 figures, 1 table, 1 algorithm</p></details> |
| **[MAGMA-Edu: Multi-Agent Generative Multimodal Framework for Text-Diagram Educational Question Generation](https://arxiv.org/abs/2511.18714v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18714v1) |  |

## AI Agent
| **Title** | **Date** | **Cool Paper** | **Comment** |
| --- | --- | --- | --- |
| **[Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution](https://arxiv.org/abs/2511.19430v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19430v1) | <details><summary>Accep...</summary><p>Accepted to AAAI 2026 (Oral). The code is available at \url{https://github.com/H-EmbodVis/GRANT}</p></details> |
| **[Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design](https://arxiv.org/abs/2511.19423v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19423v1) | 10 pages, 4 figures |
| **[Learning Robust Social Strategies with Large Language Models](https://arxiv.org/abs/2511.19405v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19405v1) |  |
| **[Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models](https://arxiv.org/abs/2508.02912v4)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2508.02912v4) | <details><summary>Publi...</summary><p>Published in the Proceedings of the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Scaling Environments for Agents (SEA). Additionally accepted for presentation in the NeurIPS 2025 Workshop: Embodied World Models for Decision Making (EWM) and the NeurIPS 2025 Workshop: Optimization for Machine Learning (OPT)</p></details> |
| **[Normative active inference: A numerical proof of principle for a computational and economic legal analytic approach to AI governance](https://arxiv.org/abs/2511.19334v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19334v1) | <details><summary>19 pa...</summary><p>19 pages, 6 figures, 1 box</p></details> |
| **[PRInTS: Reward Modeling for Long-Horizon Information Seeking](https://arxiv.org/abs/2511.19314v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19314v1) | <details><summary>18 pa...</summary><p>18 pages, code: https://github.com/G-JWLee/PRInTS</p></details> |
| **[Psychometric Tests for AI Agents and Their Moduli Space](https://arxiv.org/abs/2511.19262v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19262v1) |  |
| **[An O-RAN Framework for AI/ML-Based Localization with OpenAirInterface and FlexRIC](https://arxiv.org/abs/2511.19233v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19233v1) |  |
| **[Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?](https://arxiv.org/abs/2505.07078v4)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2505.07078v4) | <details><summary>Accep...</summary><p>Accepted to KDD 2026, Datasets & Benchmarks Track</p></details> |
| **[Agent Discovery in Internet of Agents: Challenges and Solutions](https://arxiv.org/abs/2511.19113v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19113v1) | <details><summary>This ...</summary><p>This work has been submitted to the IEEE for possible publication</p></details> |
| **[HABIT: Human Action Benchmark for Interactive Traffic in CARLA](https://arxiv.org/abs/2511.19109v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19109v1) | <details><summary>Accep...</summary><p>Accepted to WACV 2026. This is the pre-camera-ready version</p></details> |
| **[AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention](https://arxiv.org/abs/2511.18960v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18960v1) | 18 pages, 10 figures |
| **[GRAPHIC--Guidelines for Reviewing Algorithmic Practices in Human-centred Design and Interaction for Creativity](https://arxiv.org/abs/2511.17443v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.17443v2) | 20 pages, 16 figures |
| **[Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search](https://arxiv.org/abs/2511.18929v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18929v1) | 10 pages, 9 figures |
| **[Addressing Situated Teaching Needs: A Multi-Agent Framework for Automated Slide Adaptation](https://arxiv.org/abs/2511.18840v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18840v1) |  |
| **[HuggingR$^{4}$: A Progressive Reasoning Framework for Discovering Optimal Model Companions](https://arxiv.org/abs/2511.18715v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18715v1) | 19 pages, 4 figures |
| **[PresentCoach: Dual-Agent Presentation Coaching through Exemplars and Interactive Feedback](https://arxiv.org/abs/2511.15253v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.15253v2) | 13pages,6figures |
| **[RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes](https://arxiv.org/abs/2502.00392v3)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2502.00392v3) |  |
| **[TRAP: Targeted Redirecting of Agentic Preferences](https://arxiv.org/abs/2505.23518v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2505.23518v2) | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2025</p></details> |
| **[DarkMind: Latent Chain-of-Thought Backdoor in Customized LLMs](https://arxiv.org/abs/2501.18617v2)** | 2025-11-23 | [Go](https://papers.cool/arxiv/2501.18617v2) | <details><summary>19 pa...</summary><p>19 pages, 15 figures, 12 tables</p></details> |

## LLM Inference
| **Title** | **Date** | **Cool Paper** | **Comment** |
| --- | --- | --- | --- |
| **[Cost-Aware Contrastive Routing for LLMs](https://arxiv.org/abs/2508.12491v3)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2508.12491v3) |  |
| **[Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces](https://arxiv.org/abs/2511.19333v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19333v1) |  |
| **[How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective](https://arxiv.org/abs/2505.21505v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2505.21505v2) | AAAI 2026 (Oral) |
| **[WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making](https://arxiv.org/abs/2506.06725v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2506.06725v2) |  |
| **[MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization](https://arxiv.org/abs/2511.19253v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19253v1) | <details><summary>Prepr...</summary><p>Preprint. 16 pages, 6 figures. Preliminary version; extended experiments and analysis forthcoming</p></details> |
| **[Learning Plug-and-play Memory for Guiding Video Diffusion Models](https://arxiv.org/abs/2511.19229v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19229v1) |  |
| **[URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training](https://arxiv.org/abs/2505.16570v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2505.16570v2) | <details><summary>NeurI...</summary><p>NeurIPS 2025, Camera Ready</p></details> |
| **[From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation](https://arxiv.org/abs/2511.19149v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19149v1) | <details><summary>Submi...</summary><p>Submitted to Expert Systems with Applications</p></details> |
| **[TRIM: Token Reduction and Inference Modeling for Cost-Effective Language Generation](https://arxiv.org/abs/2412.07682v5)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2412.07682v5) | <details><summary>16 pa...</summary><p>16 pages, 9 tables, 5 figures</p></details> |
| **[SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression](https://arxiv.org/abs/2511.18936v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18936v1) |  |
| **[Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations](https://arxiv.org/abs/2511.18933v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18933v1) | <details><summary>20 pa...</summary><p>20 pages including appendix; technical report; NeurIPS 2024 style</p></details> |
| **[SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference](https://arxiv.org/abs/2507.06567v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2507.06567v2) | <details><summary>17 pa...</summary><p>17 pages, 11 figures. This work has been submitted to the IEEE for possible publication</p></details> |
| **[Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?](https://arxiv.org/abs/2505.22061v3)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2505.22061v3) | <details><summary>Accep...</summary><p>Accepted for EMNLP findings 2025</p></details> |
| **[MGFRec: Towards Reinforced Reasoning Recommendation with Multiple Groundings and Feedback](https://arxiv.org/abs/2510.22888v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2510.22888v2) | Accepted at KDD 2026 |
| **[KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit](https://arxiv.org/abs/2511.18868v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18868v1) | Work in progress |
| **[Think Before You Prune: Selective Self-Generated Calibration for Pruning Large Reasoning Models](https://arxiv.org/abs/2511.18864v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18864v1) | Under Review |
| **[UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model](https://arxiv.org/abs/2511.18845v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18845v1) |  |
| **[Optimizing LLM Code Suggestions: Feedback-Driven Timing with Lightweight State Bounds](https://arxiv.org/abs/2511.18842v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18842v1) | <details><summary>\c{op...</summary><p>\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses</p></details> |
| **[BiasJailbreak:Analyzing Ethical Biases and Jailbreak Vulnerabilities in Large Language Models](https://arxiv.org/abs/2410.13334v4)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2410.13334v4) | <details><summary>Accep...</summary><p>Accepted as a workshop paper at AAAI 2026</p></details> |
| **[HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations](https://arxiv.org/abs/2511.18808v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18808v1) | 12 pages |

## LLM Memory
| **Title** | **Date** | **Cool Paper** | **Comment** |
| --- | --- | --- | --- |
| **[Learning Plug-and-play Memory for Guiding Video Diffusion Models](https://arxiv.org/abs/2511.19229v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.19229v1) |  |
| **[SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression](https://arxiv.org/abs/2511.18936v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18936v1) |  |
| **[SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning](https://arxiv.org/abs/2508.06447v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2508.06447v2) |  |
| **[HERMES: Towards Efficient and Verifiable Mathematical Reasoning in LLMs](https://arxiv.org/abs/2511.18760v1)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.18760v1) |  |
| **[Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference](https://arxiv.org/abs/2511.15015v2)** | 2025-11-24 | [Go](https://papers.cool/arxiv/2511.15015v2) | 7 pages |
| **[Kitty: Accurate and Efficient 2-bit KV Cache Quantization with Dynamic Channel-wise Precision Boost](https://arxiv.org/abs/2511.18643v1)** | 2025-11-23 | [Go](https://papers.cool/arxiv/2511.18643v1) |  |
| **[Breaking Forgetting: Training-Free Few-Shot Class-Incremental Learning via Conditional Diffusion](https://arxiv.org/abs/2511.18516v1)** | 2025-11-23 | [Go](https://papers.cool/arxiv/2511.18516v1) |  |
| **[General Agentic Memory Via Deep Research](https://arxiv.org/abs/2511.18423v1)** | 2025-11-23 | [Go](https://papers.cool/arxiv/2511.18423v1) |  |
| **[Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition](https://arxiv.org/abs/2509.10729v2)** | 2025-11-23 | [Go](https://papers.cool/arxiv/2509.10729v2) | <details><summary>Prepr...</summary><p>Preprint, under review</p></details> |
| **[Think Fast: Real-Time IoT Intrusion Reasoning Using IDS and LLMs at the Edge Gateway](https://arxiv.org/abs/2511.18230v1)** | 2025-11-23 | [Go](https://papers.cool/arxiv/2511.18230v1) |  |
| **[Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse](https://arxiv.org/abs/2511.00413v2)** | 2025-11-22 | [Go](https://papers.cool/arxiv/2511.00413v2) |  |
| **[DreamGarden: A Designer Assistant for Growing Games from a Single Prompt](https://arxiv.org/abs/2410.01791v2)** | 2025-11-22 | [Go](https://papers.cool/arxiv/2410.01791v2) | <details><summary>30 pa...</summary><p>30 pages + appendix, 11 figures, published at CHI 2025</p></details> |
| **[Layer-Wise High-Impact Parameter Ratio Optimization in Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2511.17801v1)** | 2025-11-21 | [Go](https://papers.cool/arxiv/2511.17801v1) |  |
| **[Episodic Memory in Agentic Frameworks: Suggesting Next Tasks](https://arxiv.org/abs/2511.17775v1)** | 2025-11-21 | [Go](https://papers.cool/arxiv/2511.17775v1) |  |
| **[Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models](https://arxiv.org/abs/2511.11910v2)** | 2025-11-21 | [Go](https://papers.cool/arxiv/2511.11910v2) |  |
| **[SpatialGeo:Boosting Spatial Reasoning in Multimodal LLMs via Geometry-Semantics Fusion](https://arxiv.org/abs/2511.17308v1)** | 2025-11-21 | [Go](https://papers.cool/arxiv/2511.17308v1) |  |
| **[LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Design of Multi Active/Passive Core-Agent Architectures](https://arxiv.org/abs/2409.11393v3)** | 2025-11-21 | [Go](https://papers.cool/arxiv/2409.11393v3) | <details><summary>39 pa...</summary><p>39 pages, 19 figures, 3 tables. Published in Information Fusion, Volume 127, March 2026, 103865. Part of the special issue "Data Fusion Approaches in Data-Centric AI for Developing Trustworthy AI Systems"</p></details> |
| **[A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents](https://arxiv.org/abs/2511.17208v1)** | 2025-11-21 | [Go](https://papers.cool/arxiv/2511.17208v1) | Work in progress |
| **[Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation](https://arxiv.org/abs/2511.17129v1)** | 2025-11-21 | [Go](https://papers.cool/arxiv/2511.17129v1) | Accepted by AAAI'26 |
| **[MirrorMind: Empowering OmniScientist with the Expert Perspectives and Collective Knowledge of Human Scientists](https://arxiv.org/abs/2511.16997v1)** | 2025-11-21 | [Go](https://papers.cool/arxiv/2511.16997v1) | 26 pages, 4 figures |

